{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283192dd",
   "metadata": {},
   "source": [
    "# Simple Perceptron Implementation\n",
    "_by Mihai Dan Nadăș (mihai.nadas@ubbcluj.ro), January 2025_\n",
    "\n",
    "This notebook implements a version of the perceptron as introduced by Frank Rosenblatt's 1958 paper, \"The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain\". \n",
    "\n",
    "We will use basic math concepts, avoiding linear algebra (i.e., working with vectors and matrices) to the extent possible.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal is to train a model with two weights, $w_{1},\\ w_{2}$—one for each of the coordinates $x,\\ y$ of a point defined as $(x,\\ y)$—and one bias, $b$. We use an adaptation of the algebraic equation $y = mx + c$, commonly known as the slope-intercept form of a line.\n",
    "\n",
    "The model will handle a simple classification task on a linearly separable dataset based on the following function:\n",
    "\n",
    "$$\n",
    "f: \\mathbb{N} \\to \\mathbb{N}, \\quad f(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x \\bmod 2 = 0, \\\\\n",
    "2x & \\text{if } x \\bmod 2 = 1.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will first generate a dataset using the Python Standard Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe88627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "import random\n",
    "\n",
    "def generate_dataset(num_items=20, start=0, stop=100):\n",
    "    random.seed(42)\n",
    "    dataset = set()\n",
    "    \n",
    "    while len(dataset) < num_items:\n",
    "        x1 = random.randint(start, stop)\n",
    "        if x1 % 2 == 0:\n",
    "            x2 = x1\n",
    "            y = 0  # Label as Class 0 if x1 is even\n",
    "        else:\n",
    "            x2 = 2 * x1\n",
    "            y = 1  # Label as Class 1 if x1 is odd\n",
    "        dataset.add((x1, x2, y))\n",
    "    \n",
    "    return list(dataset)\n",
    "\n",
    "dataset = generate_dataset()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_ratio = 0.8\n",
    "num_train = int(len(dataset) * train_ratio)\n",
    "dataset_train, dataset_test = dataset[:num_train], dataset[num_train:]\n",
    "print(f\"Training set (n={len(dataset_train)}): {dataset_train}\")\n",
    "print(f\"Test set (n={len(dataset_test)}): {dataset_test}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af2c3d",
   "metadata": {},
   "source": [
    "## Visual Representation\n",
    "\n",
    "Using _Matplotlib_ and _pandas_, we will visually represent the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2824f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_datasets(train_dataset, test_dataset):\n",
    "    # Combine datasets into a DataFrame for easier handling\n",
    "    train_df = pd.DataFrame(train_dataset, columns=[\"x1\", \"x2\", \"class\"])\n",
    "    train_df[\"set\"] = \"Train\"\n",
    "\n",
    "    test_df = pd.DataFrame(test_dataset, columns=[\"x1\", \"x2\", \"class\"])\n",
    "    test_df[\"set\"] = \"Test\"\n",
    "\n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    # Define colors and markers\n",
    "    colors = {0: \"blue\", 1: \"red\"}\n",
    "    markers = {\"Train\": \"o\", \"Test\": \"x\"}\n",
    "\n",
    "    # Plot each group using Matplotlib\n",
    "    fig, ax = plt.subplots()\n",
    "    for (dataset, cls), group in combined_df.groupby([\"set\", \"class\"]):\n",
    "        ax.scatter(\n",
    "            group[\"x1\"],\n",
    "            group[\"x2\"],\n",
    "            color=colors[cls],\n",
    "            label=f\"{dataset} Dataset - Class {cls}\",\n",
    "            s=40,\n",
    "            marker=markers[dataset],\n",
    "            edgecolors='k'  # Add edge color for better visibility\n",
    "        )\n",
    "\n",
    "    # Manage legend and labels\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), title=\"Dataset and Class\", loc=\"best\")\n",
    "    ax.set_xlabel(\"x1\", fontsize=12)\n",
    "    ax.set_ylabel(\"x2\", fontsize=12)\n",
    "    ax.set_title(\"Training and Test Datasets\", fontsize=14)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Enhance layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "plot_datasets(dataset_train, dataset_test)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a Linear Classifier\n",
    "\n",
    "With our dataset prepared, we now turn to the mathematical foundation that enables our model to classify an input $(x_{1}, x_{2})$ as belonging to class $0$ or $1$, as follows:\n",
    "\n",
    "$$\n",
    "c: \\mathbb{N}^2 \\to \\{0,1\\}, \\quad\n",
    "c(x_{1},x_{2}) =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } (x_{1},x_{2}) \\in \\text{Class 1}, \\\\\n",
    "0, & \\text{if } (x_{1},x_{2}) \\in \\text{Class 0}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This classification can be achieved by using the algebraic representation of a line in a Cartesian coordinate system, described by:\n",
    "\n",
    "$$\n",
    "z(x_{1}, x_{2}) = w_{1}x_{1} + w_{2}x_{2} + b,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $w_{1}$ and $w_{2}$ are the weights that determine the slope, representing the angle of the resulting line relative to the $x_{1}$-axis,\n",
    "- $b$ is the bias (intercept), indicating where the line intersects the $x_{2}$-axis.\n",
    "\n",
    "From the plotted graph above, it becomes clear that the two classes are linearly separable. This makes the use of a linear separation boundary, trained using Rosenblatt's Perceptron algorithm, appropriate.\n",
    "\n",
    "To illustrate this, here's how a line defined by $w_{1}=1$, $w_{2}=0.5$, and $b=0$ would look like when plotted on our previous graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5692fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "def plot_decision_boundary(w1, w2, c, dataset_train, dataset_test):\n",
    "    # Calculate x2 based on the line equation w1*x1 + w2*x2 + c = 0\n",
    "    x2 = lambda x1: (-w1 * x1 - c) / w2 if w2 != 0 else float('inf')\n",
    "    \n",
    "    # Generate a range of x1 values from the dataset\n",
    "    x1_values = range(0, 101)\n",
    "    x2_values = [x2(x1) for x1 in x1_values]\n",
    "    \n",
    "    # Plot the datasets\n",
    "    plot_datasets(dataset_train, dataset_test)\n",
    "    \n",
    "    # Plot the decision boundary line\n",
    "    plt.plot(x1_values, x2_values, label=f\"{w1:.2f}x1 + {w2:.2f}x2 + {c:.2f} = 0\", linestyle='--', color='green')\n",
    "    \n",
    "    # Enhancements: Add title, labels, and grid for better clarity\n",
    "    plt.title(\"Plot of Datasets with Decision Boundary\")\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "# Call the updated function to plot the dataset and decision boundary\n",
    "plot_decision_boundary(-1.5, 1.1, -10, dataset_train, dataset_test)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f829b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, it is evident that in this configuration the datapoints are separated neatly. There are, however, alternative configurations for $w_{1},\\ w_{2}, \\text{ and } c$ that yield less optimal results. For example, when $w_{1}=0.1,\\ w_{2}=0.1, \\text{ and } c=0.5$, we obtain a less ideal separation boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea72906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "plot_datasets_and_zx(0.1, 0.1, 0.5)\n",
    "# As seen here, this alternative configuration provides a less ideal separation boundary.\n",
    "# Notice how many data points fall on the wrong side of the line, indicating potential misclassifications.\n",
    "# This illustrates the importance of selecting appropriate weights and biases for accurate classification.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd3c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating the Performance of the Classifier\n",
    "\n",
    "Now that we have defined $z = w_{1}x_{1} + w_{2}x_{2} + c$ as our classifier's decision boundary, and have visually demonstrated its functionality for some hand-picked values, we can now define a method to quantitatively assess its performance using the _accuracy_ metric.\n",
    "\n",
    "### Defining the Classifier\n",
    "\n",
    "Before we evaluate our classifier's performance, let's formally define it:\n",
    "\n",
    "$$\n",
    "c: \\mathbb{N} \\to \\{0, 1\\}, \\quad\n",
    "c(x_{1}, x_{2}) =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } z(x_{1}, x_{2}) \\geq 0, \\\\\n",
    "0, & \\text{if } z(x_{1}, x_{2}) < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In essence, this means that if an input $(x_{1}, x_{2})$ evaluates to a value above the decision boundary determined by $z(x_1, x_2)$, it will be classified as $1$, otherwise it will be classified as $0$.\n",
    "\n",
    "Let's implement this classifier in code and continue to discuss methods for evaluating its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c169939",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "cx = lambda x1, x2, w1, w2, c: 1 if zx(x1, x2, w1, w2, c) >= 0 else 0\n",
    "\n",
    "def accuracy(dataset, w1, w2, c):\n",
    "    correct = sum(y == cx(x1, x2, w1, w2, c) for x1, x2, y in dataset)\n",
    "    accuracy_percent = correct / len(dataset) * 100\n",
    "    print(f\"Accuracy on dataset: {correct}/{len(dataset)} correct, {accuracy_percent:.2f}% accuracy\")\n",
    "    return accuracy_percent / 100\n",
    "\n",
    "# Calculate and print accuracy for specific weights and bias on the training dataset\n",
    "train_accuracy = accuracy(dataset_train, -1.5, 1.1, -10)\n",
    "test_accuracy = accuracy(dataset_test, -1.5, 1.1, -10)\n",
    "print(f\"Training set accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "train_accuracy = accuracy(dataset_train, 0.1, 0.1, 0.5)\n",
    "test_accuracy = accuracy(dataset_test, 0.1, 0.1, 0.5)\n",
    "print(f\"Training set accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e3366",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Discussion on Accuracy\n",
    "\n",
    "As demonstrated earlier, different weights and bias values can lead to varying accuracy results. This arises because each set of weights and biases defines a unique decision boundary, which can effectively or ineffectively classify the dataset into the correct classes. The challenge is to identify the \"optimal\" values for these parameters. This is addressed through a process known as _model training_.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "Leveraging the insights obtained from our analysis, we will now embark on training our model through iterative refinements of the weights and bias values. The objective is to enhance the accuracy to an acceptable level. This iterative training allows us to discover the optimal combination of parameters that aligns with the dataset and effectively addresses the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d341089",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "# First, let's initialize the weights and bias to zero\n",
    "w1, w2, c = 0, 0, 0\n",
    "\n",
    "# Let's define the learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Create a list to store the details of the epochs\n",
    "epoch_details = []\n",
    "\n",
    "# Start the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    for x1, x2, y in dataset_train:\n",
    "        z = zx(x1, x2, w1, w2, c)\n",
    "        y_hat = 1 if z >= 0 else 0\n",
    "        w1 += learning_rate * (y - y_hat) * x1\n",
    "        w2 += learning_rate * (y - y_hat) * x2\n",
    "        c += learning_rate * (y - y_hat)\n",
    "        print(f\"  x1={x1}, x2={x2}, y={y}, z={z:.2f}, y_hat={y_hat}, w1={w1:.2f}, w2={w2:.2f}, c={c:.2f}\")\n",
    "        # Append the details to the list\n",
    "        epoch_details.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"x1\": x1,\n",
    "            \"x2\": x2,\n",
    "            \"y\": y,\n",
    "            \"z\": z,\n",
    "            \"y_hat\": y_hat,\n",
    "            \"w1\": w1,\n",
    "            \"w2\": w2,\n",
    "            \"c\": c\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the collected details\n",
    "epoch_details_df = pd.DataFrame(epoch_details)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "epoch_details_df.head()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
