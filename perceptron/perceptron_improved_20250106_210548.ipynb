{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d6095e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Perceptron Implementation\n",
    "\n",
    "_by Mihai Dan Nadăș (mihai.nadas@ubbcluj.ro), January 2025_\n",
    "\n",
    "This notebook implements a version of the perceptron inspired by Frank Rosenblatt's 1958 paper, \"The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain\".\n",
    "\n",
    "We aim to introduce basic math concepts and avoid the complexities of linear algebra as much as possible.\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook's goal is to train a model using two weights $w_{1},\\ w_{2}$, corresponding to the coordinates $x,\\ y$ of a point $(x,\\ y)$, and a bias $b$. This setup is analogous to the standard line equation $y = mx + c$, where we focus on a simple classification task. \n",
    "\n",
    "We will work with a linearly separable dataset generated by this rule:\n",
    "\n",
    "$\n",
    "f: \\mathbb{N} \\to \\mathbb{N}, \\quad f(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x \\bmod 2 = 0, \\\\\n",
    "2x & \\text{if } x \\bmod 2 = 1.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "In simpler terms, for even numbers, the function outputs the same number. For odd numbers, it doubles them.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will begin by generating a synthetic dataset using the Python Standard Library. This dataset will help us train and test our perceptron model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff51d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def generate_dataset(num_items=20, start=0, stop=100):\n",
    "    random.seed(42)\n",
    "    dataset = []\n",
    "    x1_values = set()\n",
    "    while len(dataset) < num_items:\n",
    "        x1 = random.randint(start, stop)\n",
    "        if x1 in x1_values:\n",
    "            continue\n",
    "        x1_values.add(x1)\n",
    "        x2 = x1 if x1 % 2 == 0 else 2 * x1\n",
    "        y = (\n",
    "            0 if x1 == x2 else 1\n",
    "        )  # (x1, x2) is labeled as Class 0 if x1 is even, and Class 1 otherwise\n",
    "        dataset.append((x1, x2, y))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = generate_dataset()\n",
    "\n",
    "# let's now split the dataset into training and test sets\n",
    "train_ratio = 0.8\n",
    "num_train = int(len(dataset) * train_ratio)\n",
    "dataset_train, dataset_test = dataset[:num_train], dataset[num_train:]\n",
    "print(f\"Training set (n={len(dataset_train)}): {dataset_train}\")\n",
    "print(f\"Test set (n={len(dataset_test)}: {dataset_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cfdd217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visual Representation\n",
    "\n",
    "Let's create a visual representation of the training and test datasets using _Matplotlib_ and _pandas_. Visualizations help us better understand how our data is distributed and identify any apparent patterns. We will use different colors and markers to differentiate between the classes and datasets. \n",
    "\n",
    "This visualization will provide an intuitive understanding of how our datasets look and offer insight into any linear separation between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e50bb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_datasets(train_dataset, test_dataset):\n",
    "    # Combine datasets into a DataFrame for easier handling\n",
    "    train_df = pd.DataFrame(train_dataset, columns=[\"x1\", \"x2\", \"class\"])\n",
    "    train_df[\"set\"] = \"Train\"\n",
    "\n",
    "    test_df = pd.DataFrame(test_dataset, columns=[\"x1\", \"x2\", \"class\"])\n",
    "    test_df[\"set\"] = \"Test\"\n",
    "\n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    # Define colors and markers\n",
    "    colors = {0: \"blue\", 1: \"red\"}\n",
    "    markers = {\"Train\": \"o\", \"Test\": \"x\"}\n",
    "\n",
    "    # Plot each group using Matplotlib\n",
    "    fig, ax = plt.subplots()\n",
    "    for (dataset, cls), group in combined_df.groupby([\"set\", \"class\"]):\n",
    "        ax.scatter(\n",
    "            group[\"x1\"],\n",
    "            group[\"x2\"],\n",
    "            color=colors[cls],\n",
    "            label=f\"{dataset} Dataset, Class {cls}\",\n",
    "            s=30,\n",
    "            marker=markers[dataset],\n",
    "        )\n",
    "\n",
    "    # Manage legend and labels\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), title=\"Dataset and Class\", loc=\"best\")\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    ax.set_title(\"Training and Test Datasets\")\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "plot_datasets(dataset_train, dataset_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "178c03f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Understanding Linear Classification\n",
    "\n",
    "Now that we have our dataset ready, let's explore how our model can decide if a point $(x_{1}, x_{2})$ belongs to class 0 or class 1. Picture this as a decision-making process, much like how you might decide to wear a raincoat based on the weather forecast.\n",
    "\n",
    "Given a point $(x_1, x_2)$, our task is to determine its class using a simple rule:\n",
    "\n",
    "**Classification Criteria:**\n",
    "\n",
    "If the point falls in a specific region on the graph (like the right side of a line), it belongs to one class; otherwise, it belongs to another.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "The decision boundary is expressed as a line, using the formula:\n",
    "\n",
    "$$\n",
    "z(x) = w_{1}x_{1} + w_{2}x_{2} + c\n",
    "$$\n",
    "\n",
    "- **Weights ($w_{1}$ and $w_{2}$):** Imagine adjusting these like tuning a radio. They change the line's tilt or angle on the graph.\n",
    "- **Bias ($c$):** Think of this as the shifting knob. It moves the line up or down.\n",
    "\n",
    "### Visualizing Our Method\n",
    "\n",
    "When visualizing, the line effectively slices the graph into two sections. If one side of the line is like being under an umbrella, any points there might belong to class 1; the other side belongs to class 0.\n",
    "\n",
    "**Example:** Look at how this line behaves when $w_{1}=1, w_{2}=0.5,$ and $c=0$. Can you imagine how it sits on our graph to separate the points?\n",
    "\n",
    "### Simplifying Our Goal\n",
    "\n",
    "Ultimately, what we're doing is drawing lines to separate different colored points on our plot effectively—a bit like sorting apples from oranges based on their color.\n",
    "\n",
    "Understanding this setup helps lay the groundwork for grasping more complex AI models—but for now, think of it as an art of line drawing that helps our computer \"see\" which category each point belongs to, much like label tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "zx = lambda x1, x2, w1, w2, c: w1 * x1 + w2 * x2 + c\n",
    "\n",
    "\n",
    "def plot_zx(w1, w2, c):\n",
    "    x2 = lambda x1: (\n",
    "        (-w1 * x1 - c) / w2 if w2 != 0 else -c / w1 if w1 != 0 else c\n",
    "    )  # this is because the equation of the line is w1*x1 + w2*x2 + c = 0, hence x2 = (-w1*x1 - c) / w2\n",
    "    x1_values = range(0, 101)\n",
    "    x2_values = [x2(x1) for x1 in x1_values]\n",
    "    plt.plot(x1_values, x2_values, label=f\"{w1}x1+{w2}x2+{c}=0\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def plot_datasets_and_zx(w1, w2, c):\n",
    "    plot_datasets(dataset_train, dataset_test)\n",
    "    plot_zx(w1, w2, c)\n",
    "\n",
    "\n",
    "plot_datasets_and_zx(-1.5, 1.1, -10)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "fc8c785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this configuration, the data points are clearly separated by the line. There are, however, different ways to choose $w_{1},\\ w_{2},$ and $c$ that result in less effective separations. For instance, when $w_{1}=0.1,\\ w_{2}=0.1,$ and $c=0.5$, the decision boundary becomes less efficient, demonstrating how different configurations can impact the classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datasets_and_zx(0.1, 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "890bf550",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this particular case, $z$ will not help classify any data points.\n",
    "\n",
    "## Evaluating the Performance of the Classifier\n",
    "\n",
    "Now that we have defined $z = w_{1}x_{1} + w_{2}x_{2} + c$ as our classifier's decision boundary, and visually confirmed its effectiveness for some chosen parameter values, let's develop a computational approach to measure its performance using the concept of _accuracy_.\n",
    "\n",
    "### Defining the Classifier\n",
    "\n",
    "Before assessing our classifier's performance, we need to define it mathematically:\n",
    "\n",
    "$\n",
    "c: \\mathbb{N} \\to \\{0,1\\}, \\quad\n",
    "c(x_{1},x_{2}) =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } z(x_{1},x_{2}) \\geq 0, \\\\\n",
    "0, & \\text{if } z(x_{1},x_{2}) < 0.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "This classification rule implies that if the calculated value $z(x_{1}, x_{2})$ is greater than or equal to zero, the input is classified as $1$; otherwise, it is classified as $0$.\n",
    "\n",
    "Let's move on to implement the classifier in code and subsequently discuss how to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60508e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cx = lambda x1, x2, w1, w2, c: 1 if zx(x1, x2, w1, w2, c) >= 0 else 0\n",
    "\n",
    "\n",
    "def accuracy(dataset, w1, w2, c):\n",
    "    print(f\"Calculating accuracy on training set using w1={w1}, w2={w2}, c={c}\")\n",
    "    correct = 0\n",
    "    for x1, x2, y in dataset:\n",
    "        if y == cx(x1, x2, w1, w2, c):\n",
    "            correct += 1\n",
    "    print(\n",
    "        f\"Resulting accuracy: {correct}/{len(dataset)}, or {correct/len(dataset)*100:.2f}%\"\n",
    "    )\n",
    "    return correct / len(dataset)\n",
    "\n",
    "\n",
    "# Applying the accuracy function to the training set using the two sets of weights and bias as shown above, in the first example\n",
    "accuracy(dataset_train, -1.5, 1.1, -10)\n",
    "\n",
    "# Applying the accuracy function to the training set using two sets of weights and bias as shown above, in the second example\n",
    "accuracy(dataset_train, 0.1, 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "01f30199",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Understanding Accuracy\n",
    "\n",
    "We've seen that changing weights and bias can lead to varying accuracy results. This happens because different weights and bias define distinct decision boundaries, which may classify the dataset more or less effectively. The key challenge is to identify the \"optimal\" set of parameters that best separates the data into its classes. This is achieved through a process called _model training_.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "Now, we will apply what we've learned to train our perceptron model. This involves iteratively adjusting the weights and bias to improve the accuracy. Through this process, we aim to discover the combination of parameters that most effectively categorizes our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's initialize the weights and bias to zero\n",
    "w1, w2, c = 0, 0, 0\n",
    "\n",
    "# Let's now define the learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Let's now define the number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Create a DataFrame to store the details of the epochs\n",
    "epoch_details = pd.DataFrame(columns=[\"epoch\", \"x1\", \"x2\", \"y\", \"z\", \"y_hat\", \"w1\", \"w2\", \"c\"])\n",
    "\n",
    "# Let's now start the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    for x1, x2, y in dataset_train:\n",
    "        z = zx(x1, x2, w1, w2, c)\n",
    "        y_hat = 1 if z >= 0 else 0\n",
    "        w1 += learning_rate * (y - y_hat) * x1\n",
    "        w2 += learning_rate * (y - y_hat) * x2\n",
    "        c += learning_rate * (y - y_hat)\n",
    "        print(f\"  x1={x1}, x2={x2}, y={y}, z={z:.2f}, y_hat={y_hat}, w1={w1:.2f}, w2={w2:.2f}, c={c:.2f}\")\n",
    "        # Append the details to the DataFrame\n",
    "        epoch_details = epoch_details.concat({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"x1\": x1,\n",
    "            \"x2\": x2,\n",
    "            \"y\": y,\n",
    "            \"z\": z,\n",
    "            \"y_hat\": y_hat,\n",
    "            \"w1\": w1,\n",
    "            \"w2\": w2,\n",
    "            \"c\": c\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "epoch_details.head()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
