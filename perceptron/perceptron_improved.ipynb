{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "34ac3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Perceptron Implementation\n",
    "_by Mihai Dan Nadăș (mihai.nadas@ubbcluj.ro), January 2025_\n",
    "\n",
    "This notebook implements a version of the perceptron as introduced by Frank Rosenblatt's 1958 paper, \"The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain\".\n",
    "\n",
    "We will try to use basic math concepts, avoiding linear algebra (i.e., working with vectors and matrices) to the extent possible.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal is to train a model with two weights, $w_{1},\\ w_{2}$, one for each of the coordinates $x,\\ y$ of a point defined as $(x,\\ y)$, and one bias $b$, using an adaptation of the algebraic equation $y = mx + c$ (the slope-intercept form of a line).\n",
    "\n",
    "The model will deal with a simple classification task, for a linearly separable dataset based on the following function:\n",
    "\n",
    "$\n",
    "f: \\mathbb{N} \\to \\mathbb{N}, \\quad f(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x \\bmod 2 = 0, \\\\\n",
    "2x & \\text{if } x \\bmod 2 = 1.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "## Dataset\n",
    "First, we will generate a dataset, using the Python Standard Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def generate_dataset(num_items=20, start=0, stop=100):\n",
    "    random.seed(42)\n",
    "    dataset = []\n",
    "    x1_values = set()\n",
    "    while len(dataset) < num_items:\n",
    "        x1 = random.randint(start, stop)\n",
    "        if x1 in x1_values:\n",
    "            continue\n",
    "        x1_values.add(x1)\n",
    "        x2 = x1 if x1 % 2 == 0 else 2 * x1\n",
    "        y = (\n",
    "            0 if x1 == x2 else 1\n",
    "        )  # (x1, x2) is labeled as Class 0 if x1 is even, and Class 1 otherwise\n",
    "        dataset.append((x1, x2, y))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = generate_dataset()\n",
    "\n",
    "# let's now split the dataset into training and test sets\n",
    "train_ratio = 0.8\n",
    "num_train = int(len(dataset) * train_ratio)\n",
    "dataset_train, dataset_test = dataset[:num_train], dataset[num_train:]\n",
    "print(f\"Training set (n={len(dataset_train)}): {dataset_train}\")\n",
    "print(f\"Test set (n={len(dataset_test)}: {dataset_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "e0ccec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visual Representation\n",
    "\n",
    "Using _Matplotlib_ and _pandas_, we will visually represent the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d09e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_datasets(train_dataset, test_dataset):\n",
    "    # Combine datasets into a DataFrame for easier handling\n",
    "    train_df = pd.DataFrame(train_dataset, columns=[\"x1\", \"x2\", \"class\"])\n",
    "    train_df[\"set\"] = \"Train\"\n",
    "\n",
    "    test_df = pd.DataFrame(test_dataset, columns=[\"x1\", \"x2\", \"class\"])\n",
    "    test_df[\"set\"] = \"Test\"\n",
    "\n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    # Define colors and markers\n",
    "    colors = {0: \"blue\", 1: \"red\"}\n",
    "    markers = {\"Train\": \"o\", \"Test\": \"x\"}\n",
    "\n",
    "    # Plot each group using Matplotlib\n",
    "    fig, ax = plt.subplots()\n",
    "    for (dataset, cls), group in combined_df.groupby([\"set\", \"class\"]):\n",
    "        ax.scatter(\n",
    "            group[\"x1\"],\n",
    "            group[\"x2\"],\n",
    "            color=colors[cls],\n",
    "            label=f\"{dataset} Dataset, Class {cls}\",\n",
    "            s=30,\n",
    "            marker=markers[dataset],\n",
    "        )\n",
    "\n",
    "    # Manage legend and labels\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), title=\"Dataset and Class\", loc=\"best\")\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    ax.set_title(\"Training and Test Datasets\")\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "plot_datasets(dataset_train, dataset_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "0a68c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a Linear Classifier\n",
    "\n",
    "With our dataset prepared, we now turn to the mathematical foundation that enables our model to classify an input $x_{1} \\ \\text{and} \\ x_{2}$ as belonging to classes $0$ or $1$, as follows:\n",
    "\n",
    "$\n",
    "c: \\mathbb{N} \\to \\{0,1\\}, \\quad\n",
    "c(x_{1},x_{2}) =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } (x_{1},x_{2}) \\in \\text{Class 1}, \\\\\n",
    "0, & \\text{if } (x_{1},x_{2}) \\in \\text{Class 2}.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "This classification can be achieved by using the algebraic representation of a line in a Cartesian coordinate system, described by:\n",
    "\n",
    "$\n",
    "z(x) = w_{1}x_{1}+w_{2}x_{2} + c,\n",
    "$\n",
    "\n",
    "where:\n",
    "- $w_{1} \\ \\text{and} \\ w_{2}$ represent the weights that determine the slope, which describes the angle of the resulting line relative to the $x$-axis,\n",
    "- $c$ is the intercept, indicating where the line intersects the $y$-axis.\n",
    "\n",
    "From the plotted graph above, it becomes clear that the two classes are linearly separable. This makes using a linear separation boundary, where $w_{1} \\ \\text{and} \\ w_{2}$ are trained using Rosenblatt's Perceptron algorithm, appropriate.\n",
    "\n",
    "To illustrate this concept, here's how a line defined by $w_{1}=1, \\ w_{2}=0.5, \\ \\text{and} \\ c=0$ would look when plotted on our previous graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zx = lambda x1, x2, w1, w2, c: w1 * x1 + w2 * x2 + c\n",
    "\n",
    "\n",
    "def plot_zx(w1, w2, c):\n",
    "    x2 = lambda x1: (\n",
    "        (-w1 * x1 - c) / w2 if w2 != 0 else -c / w1 if w1 != 0 else c\n",
    "    )  # this is because the equation of the line is w1*x1 + w2*x2 + c = 0, hence x2 = (-w1*x1 - c) / w2\n",
    "    x1_values = range(0, 101)\n",
    "    x2_values = [x2(x1) for x1 in x1_values]\n",
    "    plt.plot(x1_values, x2_values, label=f\"{w1}x1+{w2}x2+{c}=0\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def plot_datasets_and_zx(w1, w2, c):\n",
    "    plot_datasets(dataset_train, dataset_test)\n",
    "    plot_zx(w1, w2, c)\n",
    "\n",
    "\n",
    "plot_datasets_and_zx(-1.5, 1.1, -10)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "9d2ef9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, it is obvious that in this configuration the data points are separated neatly. However, there are alternative ways to configure $w_{1}$, $w_{2}$, and $c$ that result in less optimal examples. For instance, when $w_{1}=0.1$, $w_{2}=0.1$, and $c=0.5$, we obtain a less ideal separation boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9db10e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datasets_and_zx(0.1, 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d3b83c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this particular case, $z$ will not help classify any data points.\n",
    "\n",
    "## Evaluating the Performance of the Classifier\n",
    "\n",
    "Now that we have defined $z=w_{1}x_{1}+w_{2}x_{2}+c$ as our classifier's decision boundary, and established visually that it works for some hand-picked values (among others), let's define a computational approach to determine its performance using the _accuracy_ metric.\n",
    "\n",
    "### Defining the Classifier\n",
    "\n",
    "Before delving into the classifier's performance, let's define it:\n",
    "\n",
    "$\n",
    "c: \\mathbb{N} \\to \\{0,1\\}, \\quad\n",
    "c(x_{1},x_{2}) =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } z(x_{1}, x_{2}) \\geq 0, \\\\\n",
    "0, & \\text{if } z(x_{1}, x_{2}) < 0.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Essentially, this means that if a point lies above the decision boundary, it will be classified as $1$, otherwise as $0$.\n",
    "\n",
    "Let's implement the classifier in code and then return to the discussion about evaluating its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cx = lambda x1, x2, w1, w2, c: 1 if zx(x1, x2, w1, w2, c) >= 0 else 0\n",
    "\n",
    "\n",
    "def accuracy(dataset, w1, w2, c):\n",
    "    print(f\"Calculating accuracy on training set using w1={w1}, w2={w2}, c={c}\")\n",
    "    correct = 0\n",
    "    for x1, x2, y in dataset:\n",
    "        if y == cx(x1, x2, w1, w2, c):\n",
    "            correct += 1\n",
    "    print(\n",
    "        f\"Resulting accuracy: {correct}/{len(dataset)}, or {correct/len(dataset)*100:.2f}%\"\n",
    "    )\n",
    "    return correct / len(dataset)\n",
    "\n",
    "\n",
    "# Applying the accuracy function to the training set using the two sets of weights and bias as shown above, in the first example\n",
    "accuracy(dataset_train, -1.5, 1.1, -10)\n",
    "\n",
    "# Applying the accuracy function to the training set using two sets of weights and bias as shown above, in the second example\n",
    "accuracy(dataset_train, 0.1, 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "6d472dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Discussion on Accuracy\n",
    "\n",
    "As demonstrated earlier, different weights and bias values lead to varying accuracy results. This variation occurs because each set of weights and biases forms a distinct decision boundary, which may classify the dataset into classes correctly or incorrectly. The challenge lies in finding the \"optimal\" values for these parameters. To achieve this, we employ a process called _model training_.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "Based on the insights from our analysis, we will now proceed to train our model through multiple iterations, adjusting the weights and bias values. This iterative process helps us refine the parameters until we achieve a satisfactory level of accuracy. By doing so, we can determine the best combination of parameters that effectively classify our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79eb1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's initialize the weights and bias to zero\n",
    "w1, w2, c = 0, 0, 0\n",
    "\n",
    "# Let's now define the learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Let's now define the number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Create a DataFrame to store the details of the epochs\n",
    "epoch_details = pd.DataFrame(columns=[\"epoch\", \"x1\", \"x2\", \"y\", \"z\", \"y_hat\", \"w1\", \"w2\", \"c\"])\n",
    "\n",
    "# Let's now start the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    for x1, x2, y in dataset_train:\n",
    "        z = zx(x1, x2, w1, w2, c)\n",
    "        y_hat = 1 if z >= 0 else 0\n",
    "        w1 += learning_rate * (y - y_hat) * x1\n",
    "        w2 += learning_rate * (y - y_hat) * x2\n",
    "        c += learning_rate * (y - y_hat)\n",
    "        print(f\"  x1={x1}, x2={x2}, y={y}, z={z:.2f}, y_hat={y_hat}, w1={w1:.2f}, w2={w2:.2f}, c={c:.2f}\")\n",
    "        # Append the details to the DataFrame\n",
    "        epoch_details = epoch_details.concat({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"x1\": x1,\n",
    "            \"x2\": x2,\n",
    "            \"y\": y,\n",
    "            \"z\": z,\n",
    "            \"y_hat\": y_hat,\n",
    "            \"w1\": w1,\n",
    "            \"w2\": w2,\n",
    "            \"c\": c\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "epoch_details.head()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
