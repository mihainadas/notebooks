{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "1b1948ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Perceptron Implementation\n",
    "_by Mihai Dan Nadăș (mihai.nadas@ubbcluj.ro), January 2025_\n",
    "\n",
    "This notebook implements a version of the perceptron as introduced by Frank Rosenblatt's 1958 paper, \"The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.\"\n",
    "\n",
    "We will use basic math concepts and aim to avoid linear algebra (i.e., working with vectors and matrices) as much as possible.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal is to train a model with two weights, $w_{1}$, $w_{2}$ (one for each coordinate $x$, $y$ of a point defined as $(x, y)$), and one bias $b$, adapting the algebraic equation $y = mx + c$ (slope-intercept form of a line).\n",
    "\n",
    "The model will address a simple classification task of a linearly separable dataset based on the following function:\n",
    "\n",
    "$$\n",
    "f: \\mathbb{N} \\to \\mathbb{N}, \\quad f(x) =\n",
    "\\begin{cases}\n",
    "x, & \\text{if } x \\bmod 2 = 0, \\\\\n",
    "2x, & \\text{if } x \\bmod 2 = 1.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Dataset\n",
    "\n",
    "First, we will generate a dataset using the Python Standard Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd809a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def generate_dataset(num_items=20, start=0, stop=100):\n",
    "    random.seed(42)\n",
    "    dataset = []\n",
    "    x1_values = set()\n",
    "    while len(dataset) < num_items:\n",
    "        x1 = random.randint(start, stop)\n",
    "        if x1 in x1_values:\n",
    "            continue\n",
    "        x1_values.add(x1)\n",
    "        x2 = x1 if x1 % 2 == 0 else 2 * x1\n",
    "        y = (\n",
    "            0 if x1 == x2 else 1\n",
    "        )  # (x1, x2) is labeled as Class 0 if x1 is even, and Class 1 otherwise\n",
    "        dataset.append((x1, x2, y))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = generate_dataset()\n",
    "\n",
    "# let's now split the dataset into training and test sets\n",
    "train_ratio = 0.8\n",
    "num_train = int(len(dataset) * train_ratio)\n",
    "dataset_train, dataset_test = dataset[:num_train], dataset[num_train:]\n",
    "print(f\"Training set (n={len(dataset_train)}): {dataset_train}\")\n",
    "print(f\"Test set (n={len(dataset_test)}: {dataset_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "3aeeaad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visual Representation\n",
    "\n",
    "In this section, we will use _Matplotlib_ and _pandas_ to visually represent the training and test datasets. Visual representation helps us understand the distribution and separation of data points within different classes, and it is a crucial step in understanding our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ecbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_datasets(train_dataset, test_dataset):\n",
    "    # Combine datasets into a DataFrame for easier handling\n",
    "    train_df = pd.DataFrame(train_dataset, columns=[\"x1\", \"x2\", \"class\"])\n",
    "    train_df[\"set\"] = \"Train\"\n",
    "\n",
    "    test_df = pd.DataFrame(test_dataset, columns=[\"x1\", \"x2\", \"class\"])\n",
    "    test_df[\"set\"] = \"Test\"\n",
    "\n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    # Define colors and markers\n",
    "    colors = {0: \"blue\", 1: \"red\"}\n",
    "    markers = {\"Train\": \"o\", \"Test\": \"x\"}\n",
    "\n",
    "    # Plot each group using Matplotlib\n",
    "    fig, ax = plt.subplots()\n",
    "    for (dataset, cls), group in combined_df.groupby([\"set\", \"class\"]):\n",
    "        ax.scatter(\n",
    "            group[\"x1\"],\n",
    "            group[\"x2\"],\n",
    "            color=colors[cls],\n",
    "            label=f\"{dataset} Dataset, Class {cls}\",\n",
    "            s=30,\n",
    "            marker=markers[dataset],\n",
    "        )\n",
    "\n",
    "    # Manage legend and labels\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), title=\"Dataset and Class\", loc=\"best\")\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    ax.set_title(\"Training and Test Datasets\")\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "plot_datasets(dataset_train, dataset_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "2aa74137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a Linear Classifier\n",
    "\n",
    "With our dataset prepared, we now turn to the mathematical foundation that enables our model to classify an input $x_{1}$ and $x_{2}$ as belonging to classes $0$ or $1$. This can be formalized as follows:\n",
    "\n",
    "$c: \\mathbb{N} \\to \\{0,1\\}, \\quad c(x_{1}, x_{2}) = \\begin{cases} \n",
    "1, & \\text{if } (x_{1}, x_{2}) \\in \\text{Class 1}, \\\\\n",
    "0, & \\text{if } (x_{1}, x_{2}) \\in \\text{Class 2}.\n",
    "\\end{cases}$\n",
    "\n",
    "This classification is achieved using the algebraic equation of a line in a Cartesian coordinate system:\n",
    "\n",
    "$z(x) = w_{1}x_{1} + w_{2}x_{2} + c,$\n",
    "\n",
    "where:\n",
    "- $w_{1}$ and $w_{2}$ are the weights determining the slope or the angle of the line relative to the $x$-axis,\n",
    "- $c$ is the intercept, indicating the point where the line intersects the $y$-axis.\n",
    "\n",
    "From the plotted graph above, the two classes are linearly separable, making Rosenblatt's Perceptron algorithm suitable for determining the linear separation boundary using weights $w_{1}$ and $w_{2}$.\n",
    "\n",
    "To visualize, here's how a line defined by $w_{1}=1$, $w_{2}=0.5$, and $c=0$ would look on our earlier plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba67752",
   "metadata": {},
   "outputs": [],
   "source": [
    "zx = lambda x1, x2, w1, w2, c: w1 * x1 + w2 * x2 + c\n",
    "\n",
    "\n",
    "def plot_zx(w1, w2, c):\n",
    "    x2 = lambda x1: (\n",
    "        (-w1 * x1 - c) / w2 if w2 != 0 else -c / w1 if w1 != 0 else c\n",
    "    )  # this is because the equation of the line is w1*x1 + w2*x2 + c = 0, hence x2 = (-w1*x1 - c) / w2\n",
    "    x1_values = range(0, 101)\n",
    "    x2_values = [x2(x1) for x1 in x1_values]\n",
    "    plt.plot(x1_values, x2_values, label=f\"{w1}x1+{w2}x2+{c}=0\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def plot_datasets_and_zx(w1, w2, c):\n",
    "    plot_datasets(dataset_train, dataset_test)\n",
    "    plot_zx(w1, w2, c)\n",
    "\n",
    "\n",
    "plot_datasets_and_zx(-1.5, 1.1, -10)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "28d7e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this configuration, the datapoints are separated neatly. However, there are alternative configurations for $w_{1},\\ w_{2},$ and $c$ that result in less ideal classification. For instance, with $w_{1}=0.1,\\ w_{2}=0.1,$ and $c=0.5$, the separation boundary is not as effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad21c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datasets_and_zx(0.1, 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d9847200",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this particular case, $z$ will not help classify any data points.\n",
    "\n",
    "## Evaluating the Performance of the Classifier\n",
    "\n",
    "Now that we have defined $z = w_{1}x_{1} + w_{2}x_{2} + c$ as our classifier's decision boundary and visually confirmed its efficacy with specific values, let's define a computational approach to evaluate its performance using an accuracy metric.\n",
    "\n",
    "### Defining the Classifier\n",
    "\n",
    "Before evaluating our classifier's performance, let's define it as follows:\n",
    "\n",
    "$$\n",
    "c: \\mathbb{N} \\times \\mathbb{N} \\to \\{0,1\\}, \\quad\n",
    "c(x_{1}, x_{2}) =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } z(x_{1}, x_{2}) \\ge 0, \\\\\n",
    "0, & \\text{if } z(x_{1}, x_{2}) < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This means that if a number $x$ is above the decision boundary defined by our function, it will be classified as $1$; otherwise, it will be classified as $0$.\n",
    "\n",
    "Let's implement the classifier in code and then discuss how to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45826359",
   "metadata": {},
   "outputs": [],
   "source": [
    "cx = lambda x1, x2, w1, w2, c: 1 if zx(x1, x2, w1, w2, c) >= 0 else 0\n",
    "\n",
    "\n",
    "def accuracy(dataset, w1, w2, c):\n",
    "    print(f\"Calculating accuracy on training set using w1={w1}, w2={w2}, c={c}\")\n",
    "    correct = 0\n",
    "    for x1, x2, y in dataset:\n",
    "        if y == cx(x1, x2, w1, w2, c):\n",
    "            correct += 1\n",
    "    print(\n",
    "        f\"Resulting accuracy: {correct}/{len(dataset)}, or {correct/len(dataset)*100:.2f}%\"\n",
    "    )\n",
    "    return correct / len(dataset)\n",
    "\n",
    "\n",
    "# Applying the accuracy function to the training set using the two sets of weights and bias as shown above, in the first example\n",
    "accuracy(dataset_train, -1.5, 1.1, -10)\n",
    "\n",
    "# Applying the accuracy function to the training set using two sets of weights and bias as shown above, in the second example\n",
    "accuracy(dataset_train, 0.1, 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "fe2377ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Discussion on Accuracy\n",
    "\n",
    "Earlier, we observed that different weights and bias values lead to varying accuracy levels. This variation occurs because each set of weights and bias defines a distinct decision boundary, impacting how well they classify the dataset into correct categories. The goal is to find the \"optimal\" values for these parameters to maximize classification performance. This optimization process is called _model training_.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "Armed with the analysis insights, we will now begin training our model. This involves multiple iterations, adjusting the weights and bias values to improve accuracy. The purpose of these iterations is to discover the best combination of parameters that effectively classify our dataset, achieving enhanced accuracy for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f447c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's initialize the weights and bias to zero\n",
    "w1, w2, c = 0, 0, 0\n",
    "\n",
    "# Let's now define the learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Let's now define the number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Create a DataFrame to store the details of the epochs\n",
    "epoch_details = pd.DataFrame(columns=[\"epoch\", \"x1\", \"x2\", \"y\", \"z\", \"y_hat\", \"w1\", \"w2\", \"c\"])\n",
    "\n",
    "# Let's now start the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    for x1, x2, y in dataset_train:\n",
    "        z = zx(x1, x2, w1, w2, c)\n",
    "        y_hat = 1 if z >= 0 else 0\n",
    "        w1 += learning_rate * (y - y_hat) * x1\n",
    "        w2 += learning_rate * (y - y_hat) * x2\n",
    "        c += learning_rate * (y - y_hat)\n",
    "        print(f\"  x1={x1}, x2={x2}, y={y}, z={z:.2f}, y_hat={y_hat}, w1={w1:.2f}, w2={w2:.2f}, c={c:.2f}\")\n",
    "        # Append the details to the DataFrame\n",
    "        epoch_details = epoch_details.concat({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"x1\": x1,\n",
    "            \"x2\": x2,\n",
    "            \"y\": y,\n",
    "            \"z\": z,\n",
    "            \"y_hat\": y_hat,\n",
    "            \"w1\": w1,\n",
    "            \"w2\": w2,\n",
    "            \"c\": c\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "epoch_details.head()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
