{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "560dcd73",
   "metadata": {},
   "source": [
    "# Implementare Simplă a Perceptronului\n",
    "_de Mihai Dan Nadăș (mihai.nadas@ubbcluj.ro), Ianuarie 2025_\n",
    "\n",
    "Acest notebook implementează o versiune a perceptronului așa cum a fost introdus în lucrarea din 1958 a lui Frank Rosenblatt, \"The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain\".\n",
    "\n",
    "Ne vom concentra pe concepte matematice de bază și vom minimiza utilizarea algebrei liniare, evitând operațiile cu vectori și matrici acolo unde este posibil.\n",
    "\n",
    "## Obiectiv\n",
    "\n",
    "Obiectivul nostru este să antrenăm un model cu două greutăți, $w_{1}$ și $w_{2}$, corespunzătoare coordonatelor $x$ și $y$ ale unui punct $(x, y)$, plus o bias $b$. Acest lucru se bazează pe adaptarea ecuației algebrice $y = mx + c$, care reprezintă forma pantei-interceptului a unei drepte.\n",
    "\n",
    "Modelul va aborda o sarcină simplă de clasificare cu un set de date liniar separabil, generat conform următoarei funcții:\n",
    "\n",
    "Dat un număr întreg $x$:\n",
    "\n",
    "$\n",
    "f: \\mathbb{N} \\to \\mathbb{N}, \\quad f(x) =\n",
    "\\begin{cases}\n",
    "x, & \\text{dacă } x \\bmod 2 = 0, \\\\\n",
    "2x, & \\text{dacă } x \\bmod 2 = 1.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Această funcție clasifică numerele astfel: dacă $x$ este par, rămâne neschimbat; dacă este impar, se dublează. Această transformare va ajuta la clasificarea numerelor pe baza parității lor.\n",
    "\n",
    "## Set de date\n",
    "Vom genera un set de date folosind Biblioteca Standard Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc42bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def generate_dataset(num_items=20, start=0, stop=100):\n",
    "    random.seed(42)\n",
    "    dataset = []\n",
    "    x1_values = set()\n",
    "    while len(dataset) < num_items:\n",
    "        x1 = random.randint(start, stop)\n",
    "        if x1 in x1_values:\n",
    "            continue\n",
    "        x1_values.add(x1)\n",
    "        x2 = x1 if x1 % 2 == 0 else 2 * x1\n",
    "        y = (\n",
    "            0 if x1 == x2 else 1\n",
    "        )  # (x1, x2) este etichetat ca Clasa 0 dacă x1 este par, și Clasa 1 altfel\n",
    "        dataset.append((x1, x2, y))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = generate_dataset()\n",
    "\n",
    "# acum să împărțim setul de date în seturi de antrenament și test\n",
    "train_ratio = 0.8\n",
    "num_train = int(len(dataset) * train_ratio)\n",
    "dataset_train, dataset_test = dataset[:num_train], dataset[num_train:]\n",
    "print(f\"Set de antrenament (n={len(dataset_train)}): {dataset_train}\")\n",
    "print(f\"Set de testare (n={len(dataset_test)}: {dataset_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4eeadc",
   "metadata": {},
   "source": [
    "## Reprezentare Vizuală\n",
    "\n",
    "În această secțiune, vom vizualiza seturile de date de antrenament și testare folosind _Matplotlib_ și _pandas_. Această vizualizare ne permite să vedem clar separabilitatea claselor, care este o parte esențială a înțelegerii modului în care va funcționa algoritmul perceptronului. Prin examinarea graficelor, putem identifica frontiera de decizie liniară și evalua cât de bine pot fi clasificate datele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab935ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_datasets(train_dataset, test_dataset):\n",
    "    # Combină seturile de date într-un DataFrame pentru gestionare mai ușoară\n",
    "    train_df = pd.DataFrame(train_dataset, columns=[\"x1\", \"x2\", \"class\"])\n",
    "    train_df[\"set\"] = \"Train\"\n",
    "\n",
    "    test_df = pd.DataFrame(test_dataset, columns=[\"x1\", \"x2\", \"class\"])\n",
    "    test_df[\"set\"] = \"Test\"\n",
    "\n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    # Definire culori și markere\n",
    "    colors = {0: \"blue\", 1: \"red\"}\n",
    "    markers = {\"Train\": \"o\", \"Test\": \"x\"}\n",
    "\n",
    "    # Plotare fiecare grup folosind Matplotlib\n",
    "    fig, ax = plt.subplots()\n",
    "    for (dataset, cls), group in combined_df.groupby([\"set\", \"class\"]):\n",
    "        ax.scatter(\n",
    "            group[\"x1\"],\n",
    "            group[\"x2\"],\n",
    "            color=colors[cls],\n",
    "            label=f\"{dataset} Dataset, Class {cls}\",\n",
    "            s=30,\n",
    "            marker=markers[dataset],\n",
    "        )\n",
    "\n",
    "    # Gestionare legendă și etichete\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), title=\"Dataset and Class\", loc=\"best\")\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    ax.set_title(\"Training and Test Datasets\")\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "plot_datasets(dataset_train, dataset_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc76ee5",
   "metadata": {},
   "source": [
    "Definirea unui Clasificator Liniar\n",
    "\n",
    "Cu setul nostru de date pregătit, ne îndreptăm acum spre fundamentul matematic care permite modelului nostru să clasifice o intrare $x_{1}$ și $x_{2}$ ca aparținând claselor $0$ sau $1$. Funcția de clasificare este definită astfel:\n",
    "\n",
    "$\n",
    "c: \\mathbb{N} \\times \\mathbb{N} \\to \\{0,1\\}, \\quad\n",
    "c(x_{1}, x_{2}) =\n",
    "\\begin{cases} \n",
    "1, & \\text{dacă } (x_{1}, x_{2}) \\in \\text{Clasa 1}, \\\\\n",
    "0, & \\text{dacă } (x_{1}, x_{2}) \\in \\text{Clasa 0}.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Această clasificare se realizează utilizând reprezentarea algebrică a unei linii într-un sistem de coordonate carteziene, formulată astfel:\n",
    "\n",
    "$\n",
    "z(x) = w_{1}x_{1} + w_{2}x_{2} + c,\n",
    "$\n",
    "\n",
    "unde:\n",
    "- $w_{1}$ și $w_{2}$ sunt greutățile care determină panta, reprezentând unghiul liniei în raport cu axa $x$,\n",
    "- $c$ este biasul (sau interceptul), indicând unde intersectează linia axa $y$.\n",
    "\n",
    "Din graficul de mai sus, este evident că cele două clase sunt separabile liniar, justificând utilizarea unei frontiere de decizie liniare. Greutățile $w_{1}$ și $w_{2}$ sunt ajustate prin antrenament folosind algoritmul Perceptron al lui Rosenblatt pentru a realiza eficient această separare.\n",
    "\n",
    "Pentru o înțelegere mai clară, iată cum ar apărea o linie definită de $w_{1} = 1$, $w_{2} = 0.5$ și $c = 0$ pe graficul nostru anterior, ilustrând separarea între clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e1632",
   "metadata": {},
   "outputs": [],
   "source": [
    "zx = lambda x1, x2, w1, w2, c: w1 * x1 + w2 * x2 + c\n",
    "\n",
    "\n",
    "def plot_zx(w1, w2, c):\n",
    "    x2 = lambda x1: (\n",
    "        (-w1 * x1 - c) / w2 if w2 != 0 else -c / w1 if w1 != 0 else c\n",
    "    )  # acest lucru se datorează faptului că ecuația liniei este w1*x1 + w2*x2 + c = 0, prin urmare x2 = (-w1*x1 - c) / w2\n",
    "    x1_values = range(0, 101)\n",
    "    x2_values = [x2(x1) for x1 in x1_values]\n",
    "    plt.plot(x1_values, x2_values, label=f\"{w1}x1+{w2}x2+{c}=0\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def plot_datasets_and_zx(w1, w2, c):\n",
    "    plot_datasets(dataset_train, dataset_test)\n",
    "    plot_zx(w1, w2, c)\n",
    "\n",
    "\n",
    "plot_datasets_and_zx(-1.5, 1.1, -10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416fd39c",
   "metadata": {},
   "source": [
    "Acum, este clar că în această configurație, punctele de date sunt separate eficient. Cu toate acestea, există și alte configurații pentru $w_{1}$, $w_{2}$ și $c$ care duc la un exemplu mai puțin eficient. De exemplu, când $w_{1} = 0.1$, $w_{2} = 0.1$ și $c = 0.5$, obținem o limită de separare mai puțin optimă."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb86a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datasets_and_zx(0.1, 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17473574",
   "metadata": {},
   "source": [
    "În acest caz particular, funcția $z$ nu este eficientă în clasificarea punctelor de date, indicând că valorile curente ale ponderilor și ale bias-ului nu sunt potrivite pentru sarcină.\n",
    "\n",
    "## Evaluarea Performanței Clasificatorului\n",
    "\n",
    "Cu limita de decizie $z = w_{1}x_{1} + w_{2}x_{2} + c$ definită, următorul nostru pas este să evaluăm eficiența sa folosind o metrică numită _acuratețe_. Aceasta ne va oferi o măsură cuantificabilă despre cât de bine funcționează clasificatorul nostru pe setul de date.\n",
    "\n",
    "### Definirea Funcției Clasificatorului\n",
    "\n",
    "Înainte de a-i evalua performanța, să definim clar clasificatorul nostru. Acesta poate fi exprimat astfel:\n",
    "\n",
    "$c: \\mathbb{R}^2 \\to \\{0,1\\}, \\quad\n",
    "c(x_{1}, x_{2}) = \n",
    "\\begin{cases} \n",
    "1, & \\text{dacă } z(x_{1}, x_{2}) \\geq 0, \\\\\n",
    "0, & \\text{dacă } z(x_{1}, x_{2}) < 0.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Aceasta înseamnă că orice punct $(x_{1}, x_{2})$ care se află deasupra sau pe limita de decizie, calculată prin ecuația noastră liniară, va fi clasificat ca 1. În schimb, punctele de dedesubt vor fi clasificate ca 0.\n",
    "\n",
    "Să implementăm acest clasificator în cod, și apoi vom continua să discutăm și să-i evaluăm performanța în mai mult detaliu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cx = lambda x1, x2, w1, w2, c: 1 if zx(x1, x2, w1, w2, c) >= 0 else 0\n",
    "\n",
    "\n",
    "def accuracy(dataset, w1, w2, c):\n",
    "    print(f\"Calcul calculul acurateții pe setul de antrenament folosind w1={w1}, w2={w2}, c={c}\")\n",
    "    correct = 0\n",
    "    for x1, x2, y in dataset:\n",
    "        if y == cx(x1, x2, w1, w2, c):\n",
    "            correct += 1\n",
    "    print(\n",
    "        f\"Acuratețea rezultată: {correct}/{len(dataset)}, sau {correct/len(dataset)*100:.2f}%\"\n",
    "    )\n",
    "    return correct / len(dataset)\n",
    "\n",
    "\n",
    "# Aplicarea funcției de acuratețe la setul de antrenament folosind cele două seturi de greutăți și bias așa cum este arătat mai sus, în primul exemplu\n",
    "accuracy(dataset_train, -1.5, 1.1, -10)\n",
    "\n",
    "# Aplicarea funcției de acuratețe la setul de antrenament folosind două seturi de greutăți și bias așa cum este arătat mai sus, în al doilea exemplu\n",
    "accuracy(dataset_train, 0.1, 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b1d108",
   "metadata": {},
   "source": [
    "### Discuție despre Acuratețe\n",
    "\n",
    "După cum s-a arătat anterior, modificarea valorilor de greutăți și biasuri duce la rezultate diferite în ceea ce privește acuratețea. Acest lucru se datorează faptului că greutățile și biasurile definesc frontiera de decizie, care influențează acuratețea clasificării pe baza modului în care separă clasele din setul de date. Prin urmare, provocarea este de a descoperi valorile \"optime\" pentru acești parametri. Acest lucru se realizează printr-un proces cunoscut sub numele de _antrenare a modelului_.\n",
    "\n",
    "## Antrenarea Modelului\n",
    "\n",
    "Vom antrena acum modelul nostru folosind cunoștințele din analiza noastră. Aceasta implică o serie de iterări pentru a ajusta valorile greutăților și biasurilor până când se obține un nivel satisfăcător de acuratețe. Această metodă iterativă ne ajută să găsim cea mai bună combinație de parametri adaptată setului nostru de date și sarcinii de clasificare.\n",
    "\n",
    "### Concepte Importante în Antrenarea Modelului\n",
    "\n",
    "În cadrul antrenării modelului, anumite concepte sunt cruciale:\n",
    "\n",
    "- **Epoci:** Acest termen se referă la o trecere completă prin întregul set de date folosit pentru antrenare. Pot fi necesare mai multe epoci pentru a rafina modelul.\n",
    "- **Rata de Învățare:** Este pasul folosit la actualizarea parametrilor modelului. Aceasta determină cât de repede sau încet învață modelul.\n",
    "- **Actualizări de Greutăți:** În timpul antrenării, greutățile sunt ajustate pe baza erorii predicțiilor pentru a minimiza divergența față de etichetele adevărate.\n",
    "\n",
    "Aceste concepte joacă un rol esențial în antrenarea eficientă a modelului pentru a se asigura că generalizează bine pentru date noi. Prin iterare cu epoci și rate de învățare diferite, putem optimiza frontiera de decizie pentru rezultate mai bune în clasificare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1939027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mai întâi, să inițializăm greutățile și bias-ul la zero\n",
    "w1, w2, c = 0, 0, 0\n",
    "\n",
    "# Să definim acum rata de învățare\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Să definim acum numărul de epoci\n",
    "num_epochs = 100\n",
    "\n",
    "# Creăm un DataFrame pentru a stoca detaliile epocilor\n",
    "epoch_details = pd.DataFrame(columns=[\"epocă\", \"x1\", \"x2\", \"y\", \"z\", \"y_hat\", \"w1\", \"w2\", \"c\"])\n",
    "\n",
    "# Să începem acum bucla de antrenament\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epocă {epoch+1}\")\n",
    "    for x1, x2, y in dataset_train:\n",
    "        z = zx(x1, x2, w1, w2, c)\n",
    "        y_hat = 1 if z >= 0 else 0\n",
    "        w1 += learning_rate * (y - y_hat) * x1\n",
    "        w2 += learning_rate * (y - y_hat) * x2\n",
    "        c += learning_rate * (y - y_hat)\n",
    "        print(f\"  x1={x1}, x2={x2}, y={y}, z={z:.2f}, y_hat={y_hat}, w1={w1:.2f}, w2={w2:.2f}, c={c:.2f}\")\n",
    "        # Adăugăm detaliile la DataFrame\n",
    "        epoch_details = epoch_details.concat({\n",
    "            \"epocă\": epoch + 1,\n",
    "            \"x1\": x1,\n",
    "            \"x2\": x2,\n",
    "            \"y\": y,\n",
    "            \"z\": z,\n",
    "            \"y_hat\": y_hat,\n",
    "            \"w1\": w1,\n",
    "            \"w2\": w2,\n",
    "            \"c\": c\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Afișăm DataFrame-ul\n",
    "epoch_details.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
