{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "c93f5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Perceptron Implementation\n",
    "_by Mihai Dan Nadăș (mihai.nadas@ubbcluj.ro), January 2025_\n",
    "\n",
    "This notebook implements a version of the perceptron as introduced by Frank Rosenblatt's 1958 paper, \"The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain\".\n",
    "\n",
    "We will focus on basic mathematical concepts and minimize the use of linear algebra, avoiding vector and matrix operations where possible.\n",
    "\n",
    "## Objective\n",
    "\n",
    "Our objective is to train a model with two weights, $w_{1}$ and $w_{2}$, corresponding to the coordinates $x$ and $y$ of a point $(x, y)$, plus one bias $b$. This is based on adapting the algebraic equation $y = mx + c$, which represents the slope-intercept form of a line.\n",
    "\n",
    "The model will address a simple classification task with a linearly separable dataset, generated according to the following function:\n",
    "\n",
    "Given an integer $x$:\n",
    "\n",
    "$\n",
    "f: \\mathbb{N} \\to \\mathbb{N}, \\quad f(x) =\n",
    "\\begin{cases}\n",
    "x, & \\text{if } x \\bmod 2 = 0, \\\\\n",
    "2x, & \\text{if } x \\bmod 2 = 1.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "This function classifies numbers as follows: if $x$ is even, it remains unchanged; if odd, it doubles. This transformation will help classify the numbers based on their parity.\n",
    "\n",
    "## Dataset\n",
    "We will generate a dataset using the Python Standard Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea266fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def generate_dataset(num_items=20, start=0, stop=100):\n",
    "    random.seed(42)\n",
    "    dataset = []\n",
    "    x1_values = set()\n",
    "    while len(dataset) < num_items:\n",
    "        x1 = random.randint(start, stop)\n",
    "        if x1 in x1_values:\n",
    "            continue\n",
    "        x1_values.add(x1)\n",
    "        x2 = x1 if x1 % 2 == 0 else 2 * x1\n",
    "        y = (\n",
    "            0 if x1 == x2 else 1\n",
    "        )  # (x1, x2) is labeled as Class 0 if x1 is even, and Class 1 otherwise\n",
    "        dataset.append((x1, x2, y))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = generate_dataset()\n",
    "\n",
    "# let's now split the dataset into training and test sets\n",
    "train_ratio = 0.8\n",
    "num_train = int(len(dataset) * train_ratio)\n",
    "dataset_train, dataset_test = dataset[:num_train], dataset[num_train:]\n",
    "print(f\"Training set (n={len(dataset_train)}): {dataset_train}\")\n",
    "print(f\"Test set (n={len(dataset_test)}: {dataset_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "f71d8526",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visual Representation\n",
    "\n",
    "In this section, we will visualize the training and test datasets using _Matplotlib_ and _pandas_. This visualization allows us to clearly see the separability of the classes, which is an essential part of understanding how the perceptron algorithm will work. By examining the plots, we can identify the linear decision boundary and assess how well the data can be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_datasets(train_dataset, test_dataset):\n",
    "    # Combine datasets into a DataFrame for easier handling\n",
    "    train_df = pd.DataFrame(train_dataset, columns=[\"x1\", \"x2\", \"class\"])\n",
    "    train_df[\"set\"] = \"Train\"\n",
    "\n",
    "    test_df = pd.DataFrame(test_dataset, columns=[\"x1\", \"x2\", \"class\"])\n",
    "    test_df[\"set\"] = \"Test\"\n",
    "\n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    # Define colors and markers\n",
    "    colors = {0: \"blue\", 1: \"red\"}\n",
    "    markers = {\"Train\": \"o\", \"Test\": \"x\"}\n",
    "\n",
    "    # Plot each group using Matplotlib\n",
    "    fig, ax = plt.subplots()\n",
    "    for (dataset, cls), group in combined_df.groupby([\"set\", \"class\"]):\n",
    "        ax.scatter(\n",
    "            group[\"x1\"],\n",
    "            group[\"x2\"],\n",
    "            color=colors[cls],\n",
    "            label=f\"{dataset} Dataset, Class {cls}\",\n",
    "            s=30,\n",
    "            marker=markers[dataset],\n",
    "        )\n",
    "\n",
    "    # Manage legend and labels\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), title=\"Dataset and Class\", loc=\"best\")\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    ax.set_title(\"Training and Test Datasets\")\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "plot_datasets(dataset_train, dataset_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "9e15b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a Linear Classifier\n",
    "\n",
    "With our dataset prepared, we now turn to the mathematical foundation that enables our model to classify an input $x_{1}$ and $x_{2}$ as belonging to classes $0$ or $1$. The classification function is defined as follows:\n",
    "\n",
    "$\n",
    "c: \\mathbb{N} \\times \\mathbb{N} \\to \\{0,1\\}, \\quad\n",
    "c(x_{1}, x_{2}) =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } (x_{1}, x_{2}) \\in \\text{Class 1}, \\\\\n",
    "0, & \\text{if } (x_{1}, x_{2}) \\in \\text{Class 0}.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "This classification is achieved using the algebraic representation of a line in a Cartesian coordinate system, formulated as:\n",
    "\n",
    "$\n",
    "z(x) = w_{1}x_{1} + w_{2}x_{2} + c,\n",
    "$\n",
    "\n",
    "where:\n",
    "- $w_{1}$ and $w_{2}$ are the weights that determine the slope, representing the angle of the line relative to the $x$-axis,\n",
    "- $c$ is the bias (or intercept), indicating where the line intersects the $y$-axis.\n",
    "\n",
    "From the plotted graph above, it is evident that the two classes are linearly separable, justifying the use of a linear decision boundary. The weights $w_{1}$ and $w_{2}$ are adjusted through training with Rosenblatt's Perceptron algorithm to achieve this separation effectively.\n",
    "\n",
    "For better understanding, here's how a line defined by $w_{1} = 1$, $w_{2} = 0.5$, and $c = 0$ would appear on our previous plot, illustrating the separation between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d20f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zx = lambda x1, x2, w1, w2, c: w1 * x1 + w2 * x2 + c\n",
    "\n",
    "\n",
    "def plot_zx(w1, w2, c):\n",
    "    x2 = lambda x1: (\n",
    "        (-w1 * x1 - c) / w2 if w2 != 0 else -c / w1 if w1 != 0 else c\n",
    "    )  # this is because the equation of the line is w1*x1 + w2*x2 + c = 0, hence x2 = (-w1*x1 - c) / w2\n",
    "    x1_values = range(0, 101)\n",
    "    x2_values = [x2(x1) for x1 in x1_values]\n",
    "    plt.plot(x1_values, x2_values, label=f\"{w1}x1+{w2}x2+{c}=0\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def plot_datasets_and_zx(w1, w2, c):\n",
    "    plot_datasets(dataset_train, dataset_test)\n",
    "    plot_zx(w1, w2, c)\n",
    "\n",
    "\n",
    "plot_datasets_and_zx(-1.5, 1.1, -10)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cad909bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, it is clear that in this configuration, the datapoints are separated effectively. However, there are other configurations for $w_{1}$, $w_{2}$, and $c$ that result in a less effective example. For instance, when $w_{1} = 0.1$, $w_{2} = 0.1$, and $c = 0.5$, we obtain a less optimal separation boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4be786",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datasets_and_zx(0.1, 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a92e589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this particular case, the function $z$ is not effective in classifying the data points, indicating that the current weight and bias values are not well-suited for the task.\n",
    "\n",
    "## Evaluating the Performance of the Classifier\n",
    "\n",
    "With the decision boundary $z = w_{1}x_{1} + w_{2}x_{2} + c$ defined, our next step is to evaluate its effectiveness using a metric called _accuracy_. This will give us a quantifiable measure of how well our classifier is performing on the dataset.\n",
    "\n",
    "### Defining the Classifier Function\n",
    "\n",
    "Before assessing its performance, let's clearly define our classifier. It can be expressed as:\n",
    "\n",
    "$c: \\mathbb{R}^2 \\to \\{0,1\\}, \\quad\n",
    "c(x_{1}, x_{2}) = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } z(x_{1}, x_{2}) \\geq 0, \\\\\n",
    "0, & \\text{if } z(x_{1}, x_{2}) < 0.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "This means any point $(x_{1}, x_{2})$ that lies above or on the decision boundary, calculated by our linear equation, will be classified as 1. Conversely, points below it will be classified as 0.\n",
    "\n",
    "Let's implement this classifier in code, and then we'll proceed to discuss and evaluate its performance in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fcb448",
   "metadata": {},
   "outputs": [],
   "source": [
    "cx = lambda x1, x2, w1, w2, c: 1 if zx(x1, x2, w1, w2, c) >= 0 else 0\n",
    "\n",
    "\n",
    "def accuracy(dataset, w1, w2, c):\n",
    "    print(f\"Calculating accuracy on training set using w1={w1}, w2={w2}, c={c}\")\n",
    "    correct = 0\n",
    "    for x1, x2, y in dataset:\n",
    "        if y == cx(x1, x2, w1, w2, c):\n",
    "            correct += 1\n",
    "    print(\n",
    "        f\"Resulting accuracy: {correct}/{len(dataset)}, or {correct/len(dataset)*100:.2f}%\"\n",
    "    )\n",
    "    return correct / len(dataset)\n",
    "\n",
    "\n",
    "# Applying the accuracy function to the training set using the two sets of weights and bias as shown above, in the first example\n",
    "accuracy(dataset_train, -1.5, 1.1, -10)\n",
    "\n",
    "# Applying the accuracy function to the training set using two sets of weights and bias as shown above, in the second example\n",
    "accuracy(dataset_train, 0.1, 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "fa31d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Discussion on Accuracy\n",
    "\n",
    "As shown earlier, changing weights and bias values leads to different accuracy outcomes. This is because the weights and biases define the decision boundary, which affects classification accuracy based on how well it separates the classes in the dataset. Therefore, the challenge is to discover the \"optimal\" values for these parameters. This is achieved via a process known as _model training_.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "We will now train our model using the knowledge from our analysis. This involves a series of iterations to adjust the weights and bias values until a satisfactory level of accuracy is attained. This iterative method helps us find the best combination of parameters tailored to our dataset and classification task. \n",
    "\n",
    "### Important Concepts in Model Training\n",
    "\n",
    "In model training, certain concepts are crucial:\n",
    "\n",
    "- **Epochs:** This refers to one complete pass through the entire training dataset. Multiple epochs may be required to refine the model.\n",
    "- **Learning Rate:** This is the step size used when updating the model's parameters. It determines how quickly or slowly the model learns.\n",
    "- **Weight Updates:** During training, weights are adjusted based on the error of predictions to minimize the divergence from true labels.\n",
    "\n",
    "These concepts play a pivotal role in training the model effectively to ensure it generalizes well to new data. By iterating with different epochs and learning rates, we can optimize the decision boundary for better classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d914ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's initialize the weights and bias to zero\n",
    "w1, w2, c = 0, 0, 0\n",
    "\n",
    "# Let's now define the learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Let's now define the number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Create a DataFrame to store the details of the epochs\n",
    "epoch_details = pd.DataFrame(columns=[\"epoch\", \"x1\", \"x2\", \"y\", \"z\", \"y_hat\", \"w1\", \"w2\", \"c\"])\n",
    "\n",
    "# Let's now start the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    for x1, x2, y in dataset_train:\n",
    "        z = zx(x1, x2, w1, w2, c)\n",
    "        y_hat = 1 if z >= 0 else 0\n",
    "        w1 += learning_rate * (y - y_hat) * x1\n",
    "        w2 += learning_rate * (y - y_hat) * x2\n",
    "        c += learning_rate * (y - y_hat)\n",
    "        print(f\"  x1={x1}, x2={x2}, y={y}, z={z:.2f}, y_hat={y_hat}, w1={w1:.2f}, w2={w2:.2f}, c={c:.2f}\")\n",
    "        # Append the details to the DataFrame\n",
    "        epoch_details = epoch_details.concat({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"x1\": x1,\n",
    "            \"x2\": x2,\n",
    "            \"y\": y,\n",
    "            \"z\": z,\n",
    "            \"y_hat\": y_hat,\n",
    "            \"w1\": w1,\n",
    "            \"w2\": w2,\n",
    "            \"c\": c\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "epoch_details.head()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
